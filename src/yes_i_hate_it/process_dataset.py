"""Utilities to generate and load dataset"""
# pylint: disable = import-outside-toplevel
import sys
import logging
import numpy as np
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine

from yes_i_hate_it.gather_tweets import Tweet

from yes_i_hate_it.config import DATASET_FILE_PATH, DATASET_LOG_FILE
from yes_i_hate_it.config import TWEETS_DB_PATH
from yes_i_hate_it.config import TRAIN_PERCENTAGE


def from_text_to_array(session, text):
    """Convert given text into numpy array obtained using bow and with labels included"""
    # import required components from generate_bow just if them are required to avoid ntlk downloads
    if 'yes_i_hate_it.generate_bow.process_text' not in sys.modules:
        from yes_i_hate_it.generate_bow import process_text, Word

    total_words = session.query(Word).count()
    data = np.zeros((total_words,), dtype=np.uint8)

    processed_words = process_text(text)
    for word in processed_words:
        db_word = session.query(Word).filter(Word.text==word).first()
        data[db_word.id-1] = 1
    return data


def from_label_to_array(label):
    """Return list with labels generated by given tweet label"""
    # label = 0: not football
    # label = 1: football
    labels = {1: np.array([1, 0], dtype=np.uint8), 0: np.array([0, 1], dtype=np.uint8)}
    return labels[label]


def generate_dataset():
    """Generate dataset from sqlite database"""
    # import required components from generate_bow just if them are required to avoid ntlk downloads
    if 'yes_i_hate_it.generate_bow.process_text' not in sys.modules:
        from yes_i_hate_it.generate_bow import Word

    if not TWEETS_DB_PATH.exists():
        logging.info("Database does not exists")
        sys.exit()

    # pylint: disable = no-member
    DATASET_LOG_FILE.parents[0].mkdir(exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(DATASET_LOG_FILE),
            logging.StreamHandler()
        ]
    )

    engine = create_engine(f'sqlite:///{TWEETS_DB_PATH}')
    session_maker = sessionmaker(bind=engine)
    session = session_maker()

    # pylint: disable = singleton-comparison
    tweets = session.query(Tweet).filter(Tweet.labeled==True).all()
    total_tweets = len(tweets)
    total_words = session.query(Word).count()

    data_x = np.ndarray(shape=(total_tweets, total_words), dtype=np.uint8)
    data_y = np.ndarray(shape=(total_tweets, 2), dtype=np.uint8)

    for i, tweet in enumerate(tweets):
        data_x[i] = from_text_to_array(session, tweet.text)
        data_y[i] = from_label_to_array(tweet.is_football)

    # used to calculate slices across train (80%) and test (20%) arrays
    train_threshold = int(total_tweets * TRAIN_PERCENTAGE / 100)
    np.savez(DATASET_FILE_PATH,
        train_x=data_x[:train_threshold],
        train_y=data_y[:train_threshold],
        test_x=data_x[train_threshold:],
        test_y=data_y[train_threshold:]
    )


def load_dataset():
    """Load npz file and return train and test arrays"""
    with np.load(DATASET_FILE_PATH, allow_pickle=True) as handle:
        train_x, train_y = handle['train_x'], handle['train_y']
        test_x, test_y = handle['test_x'], handle['test_y']
    return (train_x, train_y), (test_x, test_y)


def main():
    """
    This script will be tipically only used to generated the dataset
    and the imported as other scripts to load the dataset, therefore
    main function will onyl call generate_dataset.
    """
    generate_dataset()


if __name__ == '__main__':
    main()
